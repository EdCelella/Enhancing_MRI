{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Computation Report\n",
    "\n",
    "**Reconstruction results of test data can be found [here](https://drive.google.com/drive/folders/16lBroxjaL8ddR0JEkRKtX4lUuWYkfdZ3?usp=sharing).** \n",
    "\n",
    "##  1.0 Introduction\n",
    "------\n",
    "\n",
    "Magnetic resonance imaging (MRI) is a widely used medical imaging technique, allowing the anatomical pictures to be formed through the use of magnetic fields and radio waves [1]. The uses of MRIs are countless, due to the fact it can be used to scan any part of the body. However MRIs have some drawbacks, with one of the main problems being how long the procedure takes to develop an image, which can take anywhere between 15 to 90 minutes [2]. These long procedure times create numerous problems, such as anomalies developing in the images as patients move, and a low throughput of patients who require MRI scans.\n",
    "\n",
    "This study aims to apply pioneering machine learning techniques to MRI scans, in order to speed up the procedure time required to create images. This is possible due to the way MRIs operate, as the scans do not produce real images directly, instead outputting an array of numerical values representing spatial frequencies known as k-space. This means that various adjustments can be made including, but not limited to, the spatial resolution, field of view and acquisition velocity, which will produce varying final images when changed [3]. These changes can speed up the process of an MRI scan by under-sampling the spatial frequencies, at the cost of producing a less clear image.\n",
    "\n",
    "The dataset used to conduct this study is comprised of various three-dimensional k-space volumes. Each volume can be sliced along the first dimension to obtain a two-dimensional k-space data array of size 640 by 368. Applying a Fourier transform to each slice produces a real image, which can be used by medical professionals to diagnose a patient.\n",
    "\n",
    "To simulate under sampled MRI scans, a mask is applied to the k-space slices, which reduces the spatial frequencies by removing lines (columns). The amount of columns removed by the mask is dependent on the specified simulated acceleration, which is equal to:\n",
    "\n",
    "$$\n",
    "\\frac{N}{a}\n",
    "$$\n",
    "\n",
    "Where $N$ is the amount of columns in the fully sampled k-space, and $a$ the acceleration. Each mask keeps a% of the fully sampled central region, and uses a uniform random distribution to select the remaining columns. Specifically, this study will simulate an acceleration rate and four and eight times the original fully sampled data.\n",
    "As previously stated, these under-sampled k-space slices will produce blurred real images. The goal of this study is to develop a neural network which will be able to produce images using the undersampled data, which are as close as possible to those produced from the fully sampled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Design\n",
    "------\n",
    "\n",
    "A common problem faced with deep learning methods is the curse of dimensionality. In short, as the number of dimensions in the data increases, various logical intuitions of a data structure and measures which apply in lower dimensions break down [4]. This problem is especially significant when working with image data, as many images are formations of large arrays. In the case of this study, each image in the used contains 235520 data points.\n",
    "\n",
    "A convolution neural network (CNN) is a deep learning model specifically designed to be used with image data. The general model used by CNNs is analogous with a standard artificial neural network (ANN), except for the introduction of two additional types of layers. The first new layer is the convolution layer, which uses a set of kernels to extract features from the image data. Each kernel is low in spatial dimensionality, whilst matching the depth of the input data. These kernels are then applied to the input, generating activation maps. This results in the network learning which kernels activate when a certain feature is present 5. In conjunction with this process, a ReLU activation function is used, which simply allows the activation of a neuron if the activation map passes a certain threshold. The second new layer introduced by CNNs is the pooling layer. Pooling reduces the dimensionality of the data by operating over each activation map and either taking the maximum, median or mean value, for each sub-section of the activation map (type of pooling specified on model creation) [5]. It is clear to see how these two layers retain the features of the input whilst reducing the datas dimensionality, allowing for decreased computation time and reduction in errors produced in higher dimensions.\n",
    "\n",
    "CNNs are generally used for classification of an entire image, which by definition is not the purpose of this study. Instead, the standard CNN requires augmentation to enable localised classification. One such method to enable this behaviour applied to biomedical imaging was proposed by Cire≈üan et al. [6], in which localised patches were instead classified for features. This is a more useful structure for this task, however, it should be noted that there is a trade-off between localization and context.\n",
    "\n",
    "Even with this augmentation, CNNs still remain insufficient for this task. This is due to the fact information is contracted through the network, whereas the result of this study requires the opposite effect. For this reason, a newer model, Unet, will be implemented. Unet is a model proposed by Ronneberger et al. [7], in which the entire network is composed of convolution layers. These layers are operationally identical to standard CNNs, except the final layers of the network replace the pooling layers with upsampling layers. These upsampling layers result in the opposite effect, propagating information to higher resolutions by applying a transposed convolution. A transposed convolution can operate in a variety of ways, such as distributing the value of each section of the input into its corresponding neighbourhood, or by collecting values from a region in the input layer and applying it to one value in the output [8]. In addition to this, each downsample layers result is copied to the corresponding upsampled layer. This is necessary as the context the input is lost through the downsampling, thus by overlapping the two outputs any lost information can be retained [7].\n",
    "\n",
    "This study will primarily use an implementations of different, ANN, CNN and Unet models. The model which produces the best base results will then be optimised by altering its parameter, in order to develop a model of high accuracy for the proposed task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Implementation\n",
    "------\n",
    "\n",
    "This project contains the implementation of two main models. The first is a standard CNN network, which consists of four convolution layers. This implementation is a naive approach and is simply used as a baseline in order to compare other models accuracy to. As previously described, CNNs are designed to work with image dage, resulting in a mitigation of many of the problems found when working with data of high dimensionality [4].\n",
    "\n",
    "The CNN was constructed using the pytorch library, utilising the `Conv2d` function, which simply applies a 2d convolutional of an input which consist of several planes. Each convolution layer uses a convolving kernel size of three, and performs a depthwise convolution, due to the fact:\n",
    "\n",
    "$$\n",
    "\\text{out_channels} = 2 \\times \\text{in_channels}\n",
    "$$\n",
    "\n",
    "A depthwise convolution simply means each dimension of the input volume is processed separately, which is perfect for this task as every slice is a seperate image. Each convolution layer is used in conjunction with the ReLU activation function, which filters useless output feature signals.\n",
    "\n",
    "The second model implementing within this study is the aforementioned Unet model. This builds upon the already defined CNN, with the addition of upsampling layers. These upsampling layers are defined by producing a convolution block in which:\n",
    "\n",
    "$$\n",
    "\\text{out_channels} =\\frac{ \\text{in_channels}}{2}\n",
    "$$\n",
    "\n",
    "![jupyter](img/unet.png)\n",
    "<center>Figure 1.The Unet Model [7]</center>\n",
    "\n",
    "Each of the described models is optimized through different sets of hyperparameters. Specifically the main optimization parameters used within this study are:\n",
    "\n",
    "- RMSprop: An optimisation algorithm which works similarly to gradient descent, however introduces momentum in order to converge the learning rate faster.\n",
    "- ADAM: This is another optimisation parameter which builds upon RMSprop, using the momentum average value taken from the second moment.\n",
    "- L1: This is a loss function which simply takes the sum of all the absolute value differences.\n",
    "- MSE: Another loss function which simply takes the sum of all the mean squared errors, of the residual differences.\n",
    "\n",
    "Each of these optimization and loss parameters are used with each model in order to help improve the accuracy. However, another technique used to optimize the models is through a change in the data preprocessing. Originally the data inputted into the system had little to no pre-processing, but on review of the original Unet paper [7], the results achieved were obtained through normalised data. Due to this, the data was normalised by removing sufficiently noisy slices, and applying the max-normalization function.\n",
    "\n",
    "The last main implementation of this study was the Structural Similarity Index (SSIM) function. SSIM is a classic index for scoring image quality, and can be used as loss function in neural networks to measure the quality of generated images. For this reason, SSIM will be used to evaluate each slice, with the average value computed for comparison.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Experiments\n",
    "\n",
    "-------------\n",
    "\n",
    "This section details the implementation of the different models used, as well as the data preprocessing techniques. The results gathered from each implementation are collected, analysed and then compared.\n",
    "\n",
    "The following libraries and functions have been used to load the data, conduct any preprocessing, and construct the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py, os\n",
    "from functions import transforms as T\n",
    "from functions.subsample import MaskFunc\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage.measure import compare_ssim "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SSIM function, which is used throughout the experimentation is defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssim(gt, pred):\n",
    "    \"\"\" Compute Structural Similarity Index Metric (SSIM). \"\"\"\n",
    "    return compare_ssim(\n",
    "        gt.transpose(1, 2, 0), pred.transpose(1, 2, 0), multichannel=True, data_range=gt.max()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Initial Experimentation\n",
    "\n",
    "At the beginning of the project, a few techniques which were attempted were quickly disregarded. In the initial model designs, fully connected layer were used in conjunction with convolutional layers in the model. However, fully connected layer were not be able to accurately analyse the features in the image. Additionally, for images that are 320 by 320 pixels, a lot of memory space was required to build a simple full connect layer, and save the parameters of the network. It was for these reasons that fully connected layers were removed from all models.\n",
    "\n",
    "Another method initially considered was the use of the original k-space data as input. In practice, this was quickly discovered to be an infeasible method, as the size of each image in the training data is not the same. This resulted in the images having to be cropped to ensure that the model structure was consistent. This is not possible for k-space images, as according to the principle of the k-space data, each pixel in the k-space image contributes to multiple pixels in the real image, Therefore the k-space data cannot simply be cropped, and as a result is unable to be used as an input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Training with Standard Data\n",
    "\n",
    "#### 4.2.1 Data Preprocessing\n",
    " \n",
    "The data loading part is divided into three main sections. It should be noted that a single-coil method was used, so the set of fields and attributes of data storing is based on the single-coil track."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.1.1 Loading Data\n",
    "\n",
    "Firstly, a list called 'data_list' is created to store train_data_path and val_data_path with two labels stored in train_and_val. The list is read and the training data is loaded. The shape of kspace data includes:\n",
    "\n",
    "- Number of slices.\n",
    "- Height.\n",
    "- Weight.\n",
    "\n",
    "Additionally, the first five slices are removed from the data list due to their high probability of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data path function\n",
    "def load_data_path(train_data_path, val_data_path):\n",
    "    data_list = {}\n",
    "    train_and_val = ['train', 'val']\n",
    "    data_path = [train_data_path, val_data_path]\n",
    "    for i in range(len(data_path)):\n",
    "        data_list[train_and_val[i]] = []\n",
    "        which_data_path = data_path[i]\n",
    "        for fname in sorted(os.listdir(which_data_path)):\n",
    "            subject_data_path = os.path.join(which_data_path, fname)\n",
    "            if not os.path.isfile(subject_data_path): continue\n",
    "            with h5py.File(subject_data_path, 'r') as data:\n",
    "                num_slice = data['kspace'].shape[0]\n",
    "            data_list[train_and_val[i]] += [(fname, subject_data_path, slice) for slice in range(0, num_slice)]\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.1.2 Getting Epoch Batch\n",
    "\n",
    "The following function is used to obtain tensor that can be well used in training progress. Random masks objects are defined and applied to the tensors, in which the acceleration rate is defined as four times or eight times. The `ifft2()` function is also utilises to apply an inverse Fourier Transform, in order to obtain the real image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epoch_batch(subject_id, acc, center_fract, use_seed=True):\n",
    "    fname, rawdata_name, slice = subject_id  \n",
    "    with h5py.File(rawdata_name, 'r') as data:\n",
    "        rawdata = data['kspace'][slice]            \n",
    "    slice_kspace = T.to_tensor(rawdata).unsqueeze(0)\n",
    "    S, Ny, Nx, ps = slice_kspace.shape\n",
    "    shape = np.array(slice_kspace.shape)\n",
    "    mask_func = MaskFunc(center_fractions=[center_fract], accelerations=[acc])\n",
    "    seed = None if not use_seed else tuple(map(ord, fname))\n",
    "    mask = mask_func(shape, seed)\n",
    "    masked_kspace = torch.where(mask == 0, torch.Tensor([0]), slice_kspace)\n",
    "    masks = mask.repeat(S, Ny, 1, ps)\n",
    "    img_gt, img_und = T.ifft2(slice_kspace), T.ifft2(masked_kspace)\n",
    "    rawdata_und = masked_kspace\n",
    "    return img_gt.squeeze(0), img_und.squeeze(0), rawdata_und.squeeze(0), masks.squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.1.3 MRIDataset Class Definition\n",
    "\n",
    "The implemented MRIDataset class is used as an interface to access the dataset. It includes four properties: data_list, acceleration, center_fraction, use_seed, which are assigned in constructor function. The `__len__` function simply returns the length of the data_list, and the `__getitem__` function returns a tensor of five dimensions, utilising the previously defined get_epoch_batch function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(DataLoader):\n",
    "    def __init__(self, data_list, acceleration, center_fraction, use_seed):\n",
    "        self.data_list = data_list\n",
    "        self.acceleration = acceleration\n",
    "        self.center_fraction = center_fraction\n",
    "        self.use_seed = use_seed\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subject_id = self.data_list[idx]\n",
    "        return get_epoch_batch(subject_id, self.acceleration, self.center_fraction, self.use_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Convolutional Neural Network\n",
    "\n",
    "This section describes the implementation of the first model produced for the study. It is a general CNN consisting of four convolution layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnnModel(nn.Module):\n",
    "    def __init__(self, chans):\n",
    "        super().__init__()\n",
    "        self.chans = chans\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(chans, chans * 2, kernel_size = 3 ,padding = 1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        chans *= 2\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(chans, chans * 2, kernel_size = 3 ,padding = 1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        chans *= 2\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(chans, chans // 2, kernel_size = 3 ,padding = 1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        chans //= 2\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(chans, chans // 2, kernel_size = 3 ,padding = 1),\n",
    "        )\n",
    "    def forward(self, input):\n",
    "        input = self.conv1(input)\n",
    "        input = self.conv2(input)\n",
    "        input = self.conv3(input)\n",
    "        return self.conv4(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.2.1 Training\n",
    "\n",
    "The training dataset is one of the central parts of the code. Firstly, the data path is defined for data loading of files by name, path and slices. Secondly, the parameters for the mask application are set. The acc variable is the acceleration rate, which is set equal to 4, with a centre fraction of 0.08. These values were set as a lower acceleration rate produces a sharper image, which is better for initial testing.\n",
    "\n",
    "The device parameter has been set as `cuda:0`. This decision was made due to the architecture of the GPU, which operates more effectively when performing calculations involving large data volumes, resulting in computational time-saving when compared to using a CPU. In simple terms, the core of the CPU is better at completing multiple complex tasks, focusing on logic and serial programs; whereas the core of the GPU is good at completing tasks with simple control logic, focusing on parallelism.\n",
    "\n",
    "The implementation of this model utilises the popular optimizer, Adam, with its learning rate as $1e-3$. This optimizer was selected simply due to its widespread use, in order to produce some initial data. \n",
    "\n",
    "As shown in the run model section, the _MRIDataset_ interfaced is utilised and the data processed before being called by the model. The variable input is firstly set to the first dimension, and secondly set to be the fifth dimension. The absolute value of the complex-valued tensor is then calculated, followed by the image being cropped to 320 by 320 pixels. The target variable is calculated with complex_abs and crop-size. The output gets the results trained by the network and reduce from five dimensions to four dimensions in order to maintain the same dimensions with target.\n",
    "\n",
    "The loss function used for this model, specifically the `l1_loss` function, calls output and target to compute the loss as well as average loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_train = '../train'\n",
    "data_path_val = '../train'\n",
    "data_list = load_data_path(data_path_train, data_path_val) # first load all file names, paths and slices.\n",
    "\n",
    "# set mask\n",
    "acc = 4\n",
    "cen_fract = 0.08\n",
    "seed = False # random masks for each slice \n",
    "num_workers = 0 # data loading is faster using a bigger number for num_workers. 0 means using one cpu to load data\n",
    "    \n",
    "# create data loader for training set. It applies same to validation set as well\n",
    "train_dataset = MRIDataset(data_list['train'], acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=1, num_workers=num_workers)\n",
    "    \n",
    "# set device\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# set model\n",
    "net = cnnModel(1).to(device)\n",
    "\n",
    "# set optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "    \n",
    "# run model\n",
    "crop_size = [320,320]\n",
    "EPOCHS = 20\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"epoch: \",epoch)\n",
    "    net.train()\n",
    "    avg_loss = 0.0\n",
    "    for iter, data in enumerate(train_loader):\n",
    "        gt, input, mean, std = data\n",
    "        input = T.center_crop(T.complex_abs(input.unsqueeze(1)),crop_size).to(device)\n",
    "        gt = T.center_crop(T.complex_abs(gt),crop_size).to(device)\n",
    "        output = net(input).squeeze(1)\n",
    "        loss = F.l1_loss(output, gt)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss = 0.99 * avg_loss + 0.01 * loss.item() if iter > 0 else loss.item()\n",
    "    print(\"avg_loss: \",avg_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.2.3 Evaluation\n",
    "\n",
    "After training the model, it was evaluated using the training data. The results produced showed that the model's reconstructed image performed very poorly. Initially the value of the L1 loss indicates a high level of accuracy, with a score for the final epoch of 0.00012. However, when comparing the reconstructed image is not similar to the original image. This is shown via the calculated average SSIM value, which produced a negative value of -0.19479. For this reason we can conclude two things. Firstly, a standard CNN model is not fit for the purposes of this task. Secondly the L1 loss function is not a good indicator for the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_ssim = 0\n",
    "cnt = 0\n",
    "with torch.no_grad():\n",
    "    for iter, data in enumerate(train_loader):\n",
    "        gt, input, mean, std = data\n",
    "        input = T.center_crop(T.complex_abs(input.unsqueeze(1)),crop_size).to(device)\n",
    "        gt = T.center_crop(T.complex_abs(gt),crop_size).to(device)\n",
    "        output = net(input).squeeze(1)\n",
    "        temp = ssim(gt.cpu().detach().numpy(), output.cpu().detach().numpy())\n",
    "        avg_ssim+=temp\n",
    "        cnt+=1\n",
    "print(\"avg_ssim: \",avg_ssim/cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Unet\n",
    "\n",
    "As discussed is section 2, on reviewing previous literatures and papers, Unet models performs well when applied to reconstruction tasks. Therefore, a model based on Unet was constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_chans, out_chans, drop_prob):\n",
    "        super().__init__()\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.drop_prob = drop_prob\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, out_chans, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm2d(out_chans),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(drop_prob),\n",
    "            nn.Conv2d(out_chans, out_chans, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm2d(out_chans),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(drop_prob)\n",
    "        )\n",
    "    def forward(self, input):\n",
    "        return self.layers(input)\n",
    "    def __repr__(self):\n",
    "        return f'ConvBlock(in_chans={self.in_chans}, out_chans={self.out_chans}, ' \\\n",
    "            f'drop_prob={self.drop_prob})'\n",
    "\n",
    "class UnetModel(nn.Module):\n",
    "    def __init__(self, in_chans, out_chans, chans, num_pool_layers, drop_prob):\n",
    "        super().__init__()\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.chans = chans\n",
    "        self.num_pool_layers = num_pool_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        self.down_sample_layers = nn.ModuleList([ConvBlock(in_chans, chans, drop_prob)])\n",
    "        ch = chans\n",
    "        for i in range(num_pool_layers - 1):\n",
    "            self.down_sample_layers += [ConvBlock(ch, ch * 2, drop_prob)]\n",
    "            ch *= 2\n",
    "        self.conv = ConvBlock(ch, ch, drop_prob)\n",
    "        self.up_sample_layers = nn.ModuleList()\n",
    "        for i in range(num_pool_layers - 1):\n",
    "            self.up_sample_layers += [ConvBlock(ch * 2, ch // 2, drop_prob)]\n",
    "            ch //= 2\n",
    "        self.up_sample_layers += [ConvBlock(ch * 2, ch, drop_prob)]\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(ch, ch // 2, kernel_size=1),\n",
    "            nn.Conv2d(ch // 2, out_chans, kernel_size=1),\n",
    "            nn.Conv2d(out_chans, out_chans, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        stack = []\n",
    "        output = input\n",
    "        for layer in self.down_sample_layers:\n",
    "            output = layer(output)\n",
    "            stack.append(output)\n",
    "            output = F.max_pool2d(output, kernel_size=2)\n",
    "        output = self.conv(output)\n",
    "        for layer in self.up_sample_layers:\n",
    "            output = F.interpolate(output, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            output = torch.cat([output, stack.pop()], dim=1)\n",
    "            output = layer(output)\n",
    "        return self.conv2(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.2.1 Training\n",
    "\n",
    "No changes were to the pre-processing of the training data for this model. Therefore a full explanation of the preprocessing can be found in section 4.2.2.1 of this report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path_train = '../train'\n",
    "data_path_val = '../train'\n",
    "data_list = load_data_path(data_path_train, data_path_val) # first load all file names, paths and slices.\n",
    "\n",
    "# set mask\n",
    "acc = 4\n",
    "cen_fract = 0.08\n",
    "seed = False # random masks for each slice \n",
    "num_workers = 0 # data loading is faster using a bigger number for num_workers. 0 means using one cpu to load data\n",
    "    \n",
    "# create data loader for training set. It applies same to validation set as well\n",
    "train_dataset = MRIDataset(data_list['train'], acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=1, num_workers=num_workers)\n",
    "    \n",
    "# set device\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# set model\n",
    "net = UnetModel(1,1,32,4,0.0).to(device)\n",
    "\n",
    "# set optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "    \n",
    "# run model\n",
    "crop_size = [320,320]\n",
    "EPOCHS = 20\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"epoch: \",epoch)\n",
    "    net.train()\n",
    "    avg_loss = 0.0\n",
    "    for iter, data in enumerate(train_loader):\n",
    "        gt, input, mean, std = data\n",
    "        input = T.center_crop(T.complex_abs(input.unsqueeze(1)),crop_size).to(device)\n",
    "        gt = T.center_crop(T.complex_abs(gt),crop_size).to(device)\n",
    "        output = net(input).squeeze(1)\n",
    "        loss = F.l1_loss(output, gt)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss = 0.99 * avg_loss + 0.01 * loss.item() if iter > 0 else loss.item()\n",
    "    print(\"avg_loss: \",avg_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.2.2 Evaluation\n",
    "\n",
    "After training the model, we again used the training data to verify the accuracy. The performance of the model has improved when compared to the standard CNN. But the average SSIM value is stilll low at a value of 0.04534. This is inconsistent with the experimental results outlined in the Unet paper, in which the model achieved a SSIM score of 75%. The logical conclusion of this difference is the fact the data was not normalised, as it was in the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_ssim = 0\n",
    "cnt = 0\n",
    "with torch.no_grad():\n",
    "    for iter, data in enumerate(train_loader):\n",
    "        gt, input, mean, std = data\n",
    "        input = T.center_crop(T.complex_abs(input.unsqueeze(1)),crop_size).to(device)\n",
    "        gt = T.center_crop(T.complex_abs(gt),crop_size).to(device)\n",
    "        output = net(input).squeeze(1)\n",
    "        temp = ssim(gt.cpu().detach().numpy(), output.cpu().detach().numpy())\n",
    "        avg_ssim+=temp\n",
    "        cnt+=1\n",
    "print(\"avg_ssim: \",avg_ssim/cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Training with Normalised Data\n",
    "\n",
    "#### 4.3.1 Data Preprocessing\n",
    "\n",
    "Building upon the results from the first two models, the data was preprocessed as described in the original U-net paper, through normalisation.\n",
    "\n",
    "Firstly, the first five slices were removed as the majority of the data was noise. Secondly, the remaining slices were normalised, by transforming the data into dimensionless expressions of scalars. This scales the data so that it falls into small specific intervals. In this study, the zero-filled recon is used to implement normalisation. Here, considering the features that make it easy to extract images, max-normalization was used to normalize the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_path_exclude_noise(train_data_path, val_data_path):\n",
    "    data_list = {}\n",
    "    train_and_val = ['train', 'val']\n",
    "    data_path = [train_data_path, val_data_path]\n",
    "      \n",
    "    for i in range(len(data_path)):\n",
    "\n",
    "        data_list[train_and_val[i]] = []\n",
    "        \n",
    "        which_data_path = data_path[i]\n",
    "    \n",
    "        for fname in sorted(os.listdir(which_data_path)):\n",
    "            \n",
    "            subject_data_path = os.path.join(which_data_path, fname)\n",
    "                     \n",
    "            if not os.path.isfile(subject_data_path): continue \n",
    "            \n",
    "            with h5py.File(subject_data_path, 'r') as data:\n",
    "                num_slice = data['kspace'].shape[0]\n",
    "                \n",
    "            # the first 5 slices are mostly noise so it is better to exlude them\n",
    "            data_list[train_and_val[i]] += [(fname, subject_data_path, slice) for slice in range(5, num_slice)]\n",
    "    \n",
    "    return data_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epoch_batch_with_normalization(subject_id, acc, center_fract, use_seed=True):\n",
    "    ''' random select a few slices (batch_size) from each volume'''\n",
    "\n",
    "    fname, rawdata_name, slice = subject_id  \n",
    "    \n",
    "    with h5py.File(rawdata_name, 'r') as data:\n",
    "        rawdata = data['kspace'][slice]\n",
    "                      \n",
    "    slice_kspace = T.to_tensor(rawdata).unsqueeze(0)\n",
    "    S, Ny, Nx, ps = slice_kspace.shape\n",
    "\n",
    "    # apply random mask\n",
    "    shape = np.array(slice_kspace.shape)\n",
    "    mask_func = MaskFunc(center_fractions=[center_fract], accelerations=[acc])\n",
    "    seed = None if not use_seed else tuple(map(ord, fname))\n",
    "    mask = mask_func(shape, seed)\n",
    "      \n",
    "    # undersample\n",
    "    masked_kspace = torch.where(mask == 0, torch.Tensor([0]), slice_kspace)\n",
    "    masks = mask.repeat(S, Ny, 1, ps)\n",
    "\n",
    "    img_gt, img_und = T.ifft2(slice_kspace), T.ifft2(masked_kspace)\n",
    "    #print(img_gt.shape)\n",
    "    # perform data normalization which is important for network to learn useful features\n",
    "    # during inference there is no ground truth image so use the zero-filled recon to normalize\n",
    "    norm = T.complex_abs(img_und).max()\n",
    "    if norm < 1e-6: norm = 1e-6\n",
    "    \n",
    "    # normalized data\n",
    "    img_gt, img_und, rawdata_und = img_gt/norm, img_und/norm, masked_kspace/norm\n",
    "    #print((img_gt.squeeze(0)).shape)\n",
    "    return img_gt.squeeze(0), img_und.squeeze(0), rawdata_und.squeeze(0), masks.squeeze(0), norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MRIDataset class was updated to utilise these new preprocessing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset_with_normalization(DataLoader):\n",
    "    def __init__(self, data_list, acceleration, center_fraction, use_seed):\n",
    "        self.data_list = data_list\n",
    "        self.acceleration = acceleration\n",
    "        self.center_fraction = center_fraction\n",
    "        self.use_seed = use_seed\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subject_id = self.data_list[idx]\n",
    "        return get_epoch_batch_with_normalization(subject_id, self.acceleration, self.center_fraction, self.use_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.2.1 Training\n",
    "\n",
    "The training of the model is identical to as previously described in section 4.2.2.1, with the exception that the normalised dataset is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path_train = '../train'\n",
    "data_path_val = '../train'\n",
    "data_list = load_data_path_exclude_noise(data_path_train, data_path_val) # first load all file names, paths and slices.\n",
    "\n",
    "# set mask\n",
    "acc = 4\n",
    "cen_fract = 0.08\n",
    "seed = False # random masks for each slice \n",
    "num_workers = 0 # data loading is faster using a bigger number for num_workers. 0 means using one cpu to load data\n",
    "    \n",
    "# create data loader for training set. It applies same to validation set as well\n",
    "train_dataset = MRIDataset_with_normalization(data_list['train'], acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=1, num_workers=num_workers)\n",
    "    \n",
    "# set device\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# set model\n",
    "net = cnnModel(1).to(device)\n",
    "\n",
    "# set optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "    \n",
    "# run model\n",
    "crop_size = [320,320]\n",
    "EPOCHS = 20\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"epoch: \",epoch)\n",
    "    net.train()\n",
    "    avg_loss = 0.\n",
    "    global_step = epoch * len(train_loader)\n",
    "    for iter, data in enumerate(train_loader):\n",
    "        gt, input, mean, std, norm = data\n",
    "        input = T.center_crop(T.complex_abs(input.unsqueeze(1)),crop_size).to(device)\n",
    "        gt = T.center_crop(T.complex_abs(gt),crop_size).to(device)\n",
    "        output = net(input).squeeze(1)\n",
    "        loss = F.l1_loss(output, gt)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss = 0.99 * avg_loss + 0.01 * loss.item() if iter > 0 else loss.item()\n",
    "    print(\"avg_loss: \",avg_loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.2.1 Evaluation\n",
    "\n",
    "This model obtained a higher SSIM score of 56%. However, when comparing this to the 50% SSIM of obtained by the original masked images, the model still cannot reconstruct the real images to a higher resolution. This value was achieved, after three optimizations of the model, thus a conclusion can be drawn that the normalized data greatly improves the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_ssim = 0\n",
    "cnt = 0\n",
    "with torch.no_grad():\n",
    "    for iter, data in enumerate(train_loader):\n",
    "        gt, input, mean, std, norm = data\n",
    "        input = T.center_crop(T.complex_abs(input.unsqueeze(1)),crop_size).to(device)\n",
    "        gt = T.center_crop(T.complex_abs(gt),crop_size).to(device)\n",
    "        output = net(input).squeeze(1)\n",
    "        temp = ssim(gt.cpu().detach().numpy(), output.cpu().detach().numpy())\n",
    "        avg_ssim+=temp\n",
    "        cnt+=1\n",
    "print(\"avg_ssim: \",avg_ssim/cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.3 Unet\n",
    "\n",
    "This model was the last developed in this study, and is the culmination of results gathered from experimenting with the previous models. The decision to primarily focus on this model is twofold:\n",
    "\n",
    "- Between the Unet and CNN model results from the standard data, achieving a SIMM score of 0.24013 better than the standard CNN.\n",
    "- Normalisation of the data produced a vast increase in accuracy for the CNN model.\n",
    "\n",
    "Four main iterations of this model, using the normalised data, were developed. All of which are detailed in this section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_train = '../train'\n",
    "data_path_val = '../train'\n",
    "data_list = load_data_path_exclude_noise(data_path_train, data_path_val) # first load all file names, paths and slices.\n",
    "\n",
    "# set mask\n",
    "acc = 4\n",
    "cen_fract = 0.08\n",
    "seed = False # random masks for each slice \n",
    "num_workers = 0 # data loading is faster using a bigger number for num_workers. 0 means using one cpu to load data\n",
    "    \n",
    "# create data loader for training set. It applies same to validation set as well\n",
    "train_dataset = MRIDataset_with_normalization(data_list['train'], acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=1, num_workers=num_workers)\n",
    "    \n",
    "# set device\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.3.2 Splitting the Dataset\n",
    "\n",
    "The data was split into two sections via random sampling:\n",
    "\n",
    "- 80% of the data will form the training set.\n",
    "- 20% of the data will form the validation set.\n",
    "\n",
    "The reason for this split is to obtain results from unseen data. Thus giving a more accurate representation of the SSIM scores, as well as alert us to any overfitting of the data that maybe occurring.\n",
    "These datasets will be used in every subsequent iteration of the model to train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_train = '../train_80'\n",
    "data_path_val = '../val_20'\n",
    "data_list = load_data_path_exclude_noise(data_path_train, data_path_val) # first load all file names, paths and slices.\n",
    "\n",
    "# set mask\n",
    "acc = 4\n",
    "cen_fract = 0.08\n",
    "seed = False # random masks for each slice \n",
    "num_workers = 0 # data loading is faster using a bigger number for num_workers. 0 means using one cpu to load data\n",
    "    \n",
    "# create data loader for training set. It applies same to validation set as well\n",
    "train_dataset = MRIDataset_with_normalization(data_list['train'], acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "val_dataset = MRIDataset_with_normalization(data_list['val'], acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=1, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_dataset, shuffle=True, batch_size=1, num_workers=num_workers) \n",
    "\n",
    "# set device\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.3.2 Iteration One\n",
    "\n",
    "In the first iteration, `RMSprop` was used as the optimizer and `L1` was retained as the loss function. The learning rate of optimizer is set to 1e-3,  and reduced by a rate of 10 for every 40 epochs. The model will run for 50 epochs total, with the hope being that the model can achieve better convergence in the first 40 training epochs, and the last 10 epochs will be used to simply optimize around the local minimum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model\n",
    "net = UnetModel(1,1,32,4,0.0).to(device)\n",
    "\n",
    "# set optimizer\n",
    "optimizer = torch.optim.RMSprop(net.parameters(), lr=1e-3)\n",
    "    \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 40, 0.1)\n",
    "    \n",
    "# run model\n",
    "crop_size = [320,320]\n",
    "EPOCHS = 50\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"epoch: \",epoch)\n",
    "    net.train()\n",
    "    avg_loss = 0.\n",
    "    global_step = epoch * len(train_loader)\n",
    "    for iter, data in enumerate(train_loader):\n",
    "        gt, input, mean, std, norm = data\n",
    "        input = T.center_crop(T.complex_abs(input.unsqueeze(1)),crop_size).to(device)\n",
    "        gt = T.center_crop(T.complex_abs(gt),crop_size).to(device)\n",
    "        output = net(input).squeeze(1)\n",
    "        loss = F.l1_loss(output, gt)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss = 0.99 * avg_loss + 0.01 * loss.item() if iter > 0 else loss.item()\n",
    "    print(\"avg_loss: \",avg_loss)\n",
    "    \n",
    "torch.save(net,'../trainedmodel/Fourth_generation_model_all_train_dataset.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Using the validation set, the model still achieved a good performance, with a SSIM of around 61%. Thus showing that the model did not overfit the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model\n",
    "net = UnetModel(1,1,32,4,0.0).to(device)\n",
    "\n",
    "# set optimizer\n",
    "optimizer = torch.optim.RMSprop(net.parameters(), lr=1e-3)\n",
    "    \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 40, 0.1)\n",
    "    \n",
    "# run model\n",
    "crop_size = [320,320]\n",
    "EPOCHS = 50\n",
    "\n",
    "# save loss and ssim value\n",
    "feloss = list()\n",
    "fessim = list()\n",
    "x = np.arange(0,EPOCHS,1)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"epoch: \",epoch)\n",
    "    scheduler.step(epoch)\n",
    "    net.train()\n",
    "    avg_loss = 0.0\n",
    "    global_step = epoch * len(train_loader)\n",
    "    for iter, data in enumerate(train_loader):\n",
    "        gt, input, mean, std, norm = data\n",
    "        input = T.center_crop(T.complex_abs(input.unsqueeze(1)),crop_size).to(device)\n",
    "        gt = T.center_crop(T.complex_abs(gt),crop_size).to(device)\n",
    "        output = net(input).squeeze(1)\n",
    "        loss = F.l1_loss(output, gt)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss = 0.99 * avg_loss + 0.01 * loss.item() if iter > 0 else loss.item()\n",
    "    print(\"avg_loss: \",avg_loss)\n",
    "    feloss.append(avg_loss)\n",
    "    avg_ssim = 0\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for iter, data in enumerate(val_loader):\n",
    "            gt, input, mean, std, norm = data\n",
    "            input = T.center_crop(T.complex_abs(input.unsqueeze(1)),crop_size).to(device)\n",
    "            gt = T.center_crop(T.complex_abs(gt),crop_size).to(device)\n",
    "            output = net(input).squeeze(1)\n",
    "            avg_ssim += ssim(gt.cpu().detach().numpy(), output.cpu().detach().numpy())\n",
    "            cnt+=1\n",
    "    print(\"avg_ssim: \",avg_ssim/cnt)\n",
    "    fessim.append(avg_ssim/cnt)\n",
    "\n",
    "plt.plot(x,feloss,'go-')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('ssim')\n",
    "plt.plot(x,fessim,'go-')\n",
    "plt.show()\n",
    "\n",
    "torch.save(net,'../trainedmodel/First_example.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.3.3 Iteration Two\n",
    "\n",
    "The second iteration kept the optimizer and its hyperparameters. But the loss function  was changed to MSE, to evaluate whether it would improve the accuracy of the model. This yielded no change in result, with the SSIM score remaining at 61%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model\n",
    "net = UnetModel(1,1,32,4,0.0).to(device)\n",
    "\n",
    "# set optimizer\n",
    "optimizer = torch.optim.RMSprop(net.parameters(), lr=1e-3)\n",
    "    \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 40, 0.1)\n",
    "    \n",
    "# run model\n",
    "crop_size = [320,320]\n",
    "EPOCHS = 50\n",
    "\n",
    "# save loss and ssim value\n",
    "seloss = list()\n",
    "sessim = list()\n",
    "x = np.arange(0,EPOCHS,1)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"epoch: \",epoch)\n",
    "    scheduler.step(epoch)\n",
    "    net.train()\n",
    "    avg_loss = 0.\n",
    "    global_step = epoch * len(train_loader)\n",
    "    for iter, data in enumerate(train_loader):\n",
    "        gt, input, mean, std, norm = data\n",
    "        input = T.center_crop(T.complex_abs(input.unsqueeze(1)),crop_size).to(device)\n",
    "        gt = T.center_crop(T.complex_abs(gt),crop_size).to(device)\n",
    "        output = net(input).squeeze(1)\n",
    "        loss_fn = torch.nn.MSELoss(reduce=True, size_average=False)\n",
    "        loss = loss_fn(output, gt)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss = 0.99 * avg_loss + 0.01 * loss.item() if iter > 0 else loss.item()\n",
    "    print(\"avg_loss: \",avg_loss)\n",
    "    seloss.append(avg_loss)\n",
    "    avg_ssim = 0\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for iter, data in enumerate(val_loader):\n",
    "            gt, input, mean, std, norm = data\n",
    "            input = T.center_crop(T.complex_abs(input.unsqueeze(1)),crop_size).to(device)\n",
    "            gt = T.center_crop(T.complex_abs(gt),crop_size).to(device)\n",
    "            output = net(input).squeeze(1)\n",
    "            avg_ssim += ssim(gt.cpu().detach().numpy(), output.cpu().detach().numpy())\n",
    "            cnt+=1\n",
    "    print(\"avg_ssim: \",avg_ssim/cnt)\n",
    "    sessim.append(avg_ssim/cnt)\n",
    "    \n",
    "\n",
    "plt.plot(x,seloss,'go-')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n",
    "plt.plot(x,sessim,'go-')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('ssim')\n",
    "plt.show()\n",
    "\n",
    "torch.save(net,'../trainedmodel/Second_example.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.3.4 Iteration Three\n",
    "\n",
    "The third iteration of the Unet model, changed the optimizer to Adam, and retained the L1 loss function. The learning rate was kept at $1e-3$, with the learning rate reduced by a factor of 10 every 40 epochs (the reason for this was explained in section 4.3.3.2).\n",
    "\n",
    "When using Adam as an optimizer, the SSIM rises quickly, reaching the upper limit of model accuracy. Alas there is still no improvement in the accuracy of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model\n",
    "net = UnetModel(1,1,32,4,0.0).to(device)\n",
    "\n",
    "# set optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "    \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 40, 0.1)\n",
    "    \n",
    "# run model\n",
    "crop_size = [320,320]\n",
    "EPOCHS = 50\n",
    "\n",
    "# save loss and ssim value\n",
    "teloss = list()\n",
    "tessim = list()\n",
    "x = np.arange(0,EPOCHS,1)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"epoch: \",epoch)\n",
    "    scheduler.step(epoch)\n",
    "    net.train()\n",
    "    avg_loss = 0.0\n",
    "    global_step = epoch * len(train_loader)\n",
    "    for iter, data in enumerate(train_loader):\n",
    "        gt, input, mean, std, norm = data\n",
    "        input = T.center_crop(T.complex_abs(input.unsqueeze(1)),crop_size).to(device)\n",
    "        gt = T.center_crop(T.complex_abs(gt),crop_size).to(device)\n",
    "        output = net(input).squeeze(1)\n",
    "        loss = F.l1_loss(output, gt)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss = 0.99 * avg_loss + 0.01 * loss.item() if iter > 0 else loss.item()\n",
    "    print(\"avg_loss: \",avg_loss)\n",
    "    teloss.append(avg_loss)\n",
    "    avg_ssim = 0\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for iter, data in enumerate(val_loader):\n",
    "            gt, input, mean, std, norm = data\n",
    "            input = T.center_crop(T.complex_abs(input.unsqueeze(1)),crop_size).to(device)\n",
    "            gt = T.center_crop(T.complex_abs(gt),crop_size).to(device)\n",
    "            output = net(input).squeeze(1)\n",
    "            avg_ssim += ssim(gt.cpu().detach().numpy(), output.cpu().detach().numpy())\n",
    "            cnt+=1\n",
    "    print(\"avg_ssim: \",avg_ssim/cnt)\n",
    "    tessim.append(avg_ssim/cnt)\n",
    "    \n",
    "\n",
    "plt.plot(x,teloss,'go-')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n",
    "plt.plot(x,tessim,'go-')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('ssim')\n",
    "plt.show()\n",
    "\n",
    "torch.save(net,'../trainedmodel/Third_example.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.3.5 Iteration Four\n",
    "The final iteration of the model, again using the Adam optimizer and its hyperparameters, but pairing it with the MSE loss function. However this again saw little effect, still achieving a SSIM score 60%. This could be due to a variety of reasons, including design and training times of the Unet model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model\n",
    "net = UnetModel(1,1,32,4,0.0).to(device)\n",
    "\n",
    "# set optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "    \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 40, 0.1)\n",
    "    \n",
    "# run model\n",
    "crop_size = [320,320]\n",
    "EPOCHS = 50\n",
    "\n",
    "# save loss and ssim value\n",
    "foeloss = list()\n",
    "foessim = list()\n",
    "x = np.arange(0,EPOCHS,1)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"epoch: \",epoch)\n",
    "    scheduler.step(epoch)\n",
    "    net.train()\n",
    "    avg_loss = 0.\n",
    "    global_step = epoch * len(train_loader)\n",
    "    for iter, data in enumerate(train_loader):\n",
    "        gt, input, mean, std, norm = data\n",
    "        input = T.center_crop(T.complex_abs(input.unsqueeze(1)),crop_size).to(device)\n",
    "        gt = T.center_crop(T.complex_abs(gt),crop_size).to(device)\n",
    "        output = net(input).squeeze(1)\n",
    "        loss_fn = torch.nn.MSELoss(reduce=True, size_average=False)\n",
    "        loss = loss_fn(output, gt)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss = 0.99 * avg_loss + 0.01 * loss.item() if iter > 0 else loss.item()\n",
    "    print(\"avg_loss: \",avg_loss)\n",
    "    foeloss.append(avg_loss)\n",
    "    avg_ssim = 0\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for iter, data in enumerate(val_loader):\n",
    "            gt, input, mean, std, norm = data\n",
    "            input = T.center_crop(T.complex_abs(input.unsqueeze(1)),crop_size).to(device)\n",
    "            gt = T.center_crop(T.complex_abs(gt),crop_size).to(device)\n",
    "            output = net(input).squeeze(1)\n",
    "            avg_ssim += ssim(gt.cpu().detach().numpy(), output.cpu().detach().numpy())\n",
    "            cnt+=1\n",
    "    print(\"avg_ssim: \",avg_ssim/cnt)\n",
    "    foessim.append(avg_ssim/cnt)\n",
    "    \n",
    "\n",
    "plt.plot(x,foeloss,'go-')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n",
    "plt.plot(x,foessim,'go-')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('ssim')\n",
    "plt.show()\n",
    "\n",
    "torch.save(net,'../trainedmodel/Fourth_example.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.3.6 Summary\n",
    "\n",
    "Obviously, when using the RMSprop optimizer, the loss of the model drops rapidly, but eventually converges to a certain value range like Adam. We can also find that the loss of MSE is more steadily reduced in the MRI dataset.\n",
    "Even if the performance of the four examples is slightly different, this still shows that the model needs further optimization to improve its accuracy. Simply changing the optimizer and loss function cannot meet the requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,feloss,'go-')\n",
    "plt.plot(x,teloss,'ro-')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('L1 loss')\n",
    "plt.show()\n",
    "plt.plot(x,seloss,'bo-')\n",
    "plt.plot(x,foeloss,'yo-')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('MSEloss')\n",
    "plt.show()\n",
    "plt.plot(x,fessim,'go-')\n",
    "plt.plot(x,sessim,'bo-')\n",
    "plt.plot(x,tessim,'ro-')\n",
    "plt.plot(x,foessim,'yo-')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('ssim')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Conclusion\n",
    "___\n",
    "This study outlines the use of two models, a standard CNN network, and the augmented CNN network Unet. Both of these models were run twice, using a different preprocessing technique for each.\n",
    "\n",
    "The first model, the general CNN, did not perform well using the un-normalised data. Achieving a SSIM result of -19%. Thus producing a worse result than the original blurred image inputted into the system. The Unet model achieved a much better result, scoring 4.5%. However, this result is still far from desirable.\n",
    "\n",
    "The biggest gains in accuracy achieved was through the application of the maximum nominalization method, which normalised the input the data. This saw massive improvements to the general CNN model, reaching a top accuracy of 56%. Although the leap was impressive, the end result is still no better than the actual blurred input image.\n",
    "\n",
    "By consulting the results gathered from training both the CNN and Unet with the standard image data, and the massive improvements caused by normalising the input data, it was concluded that the best model performance would come from a combination of Unet and normalised data. This proved to be true, as the network scored 61% SSIM score.\n",
    "This model was tested with a variety of parameter changes in an attempt to improve the score. To this end both the Adam and RMSprop functions, as well as the L1 and MSE loss functions. Each parameter was replaced in turn to evaluate the effects caused by altering that certain hyperparameter. Although no improvements to the results were made, some interesting observations can be made in regards to the training effects of these hyperparameters:\n",
    "\n",
    "- Both loss functions have roughly the same convergence.\n",
    "- All optimisers have the same rate of change to the SSIM score.\n",
    "\n",
    "A conclusion can therefore be made that the optimizer and loss function have no impact on the results. It should also be noted that the SSIM function converges to what seems to be a ceiling limit of between 60-70%. This indicates that the model itself is requires structural change. Future work should test how the addition of layers affects the overall accuracy of the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "-----\n",
    "[^1]: dictionary.cambridge.org, \"MRI\", 2019. Available: https://dictionary.cambridge.org/dictionary/english/mri  \n",
    "[^2]: nhs.uk, \"Overview - MRI scan\", 2018. `Available:`https://www.nhs.uk/conditions/mri-scan/.  \n",
    "[^3]: D. Moratal, A. Vall√©s-Luch, L. Mart√≠-Bonmat√≠ and ME. Brummer, \"k-Space tutorial: an MRI educational tool for a better understanding of k-space\" in _Biomedical Imaging and Intervention Journal_, vol. 4(1), e15, Jan 2008.  \n",
    "[^4]:Keogh E. and Mueen A., \"Curse of Dimensionality\", in _Sammut C., Webb G.I. (eds) Encyclopedia of Machine Learning and Data Mining_, Springer, Boston, MA, 2017.  \n",
    "[^5]: K. O‚ÄôShea1 and R. Nash, \"An Introduction to Convolutional Neural Networks\" in _Cornell University arxiv.org_, eprint 1511.08458, 2015  \n",
    "[^6]:D.C. Ciresan, L.M. Gambardella, A. Giusti and J. Schmidhuber, \"Deep neural net- works segment neuronal membranes in electron microscopy images\", in _Neural Information Processing Systems - NIPS_, pp. 2852‚Äì2860, 2012  \n",
    "[^7]: O. Ronneberger, P. Fischer, and T. Brox, \"U-Net: Convolutional Networks for Biomedical Image Segmentation\" in _Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2015_, N. Navab, J. Hornegger, WM. Wells, and AF. Frangi, Cham: Springer International Publishing, pp. 234-241, 2015  \n",
    "[^8]: V. Dumoulin and F. Visin, \"A guide to convolution arithmetic for deep learning\" in _Cornell University arxiv.org_, eprint 1603.07285, 2016   \n",
    "[^9]: Ronneberger, Olaf, Philipp Fischer and Thomas Brox. ‚ÄúU-Net: Convolutional Networks for Biomedical Image Segmentation.‚Äù ArXiv abs/1505.04597 (2015): n. pag.  \n",
    "[^10]: The HDF Group,\"HDF5 Image (H5IM) Interface\",hdfgroup.org,2017. Available: https://portal.hdfgroup.org/display/HDF5/HDF5+Image+%28H5IM%29+Interface [Accessed 10 Dec. 2019].   \n",
    "[^11]: Zbontar, Jure, et al. \"fastmri: An open dataset and benchmarks for accelerated mri.\" arXiv preprint arXiv:1811.08839 (2018).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
